{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f56c16c6",
   "metadata": {},
   "source": [
    "## Solução baseada em 2 artigos:\n",
    "\n",
    "- https://medium.com/trustyou-engineering/topic-modelling-with-pyspark-and-spark-nlp-a99d063f1a6e\n",
    "- https://medium.com/analytics-vidhya/distributed-topic-modelling-using-spark-nlp-and-spark-mllib-lda-6db3f06a4da3\n",
    "\n",
    "# Como foi feito\n",
    "\n",
    "O objetivo é descobrir os principais tópicos das revisões presentes nos dados. Uma ideia inicial era apenas observar as palavras mais frequentes, porém, em algumas pesquisas, encontramos um algoritmo de modelagem de tópicos chamado de _LDA (Latent Dirichlet Allocation)_, que de forma resumida, agrupa palavras com maior probabilidade de aparecerem juntas em tópicos, em outras palavras, essa técnica é capaz de \"deduzir\" os tópicos presentes em um corpo de texto qualquer, utilizando probabilidade.\n",
    "\n",
    "Por essa razão, utilizamos esta técnica, pois informamos o número desejado de tópicos e o algoritmo irá detectar esses n tópicos. O número de tópicos é vital, pois ao optar por um número grande de tópicos, você pode ter tópicos com muitas palavras em comum, contudo, menos tópicos também pode implicar no seu tópico ser geral demais e portanto não fornecer nenhuma informação útil para análise.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2579a3f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/ec2-user/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ec2-user/.ivy2/cache"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/08 20:23:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "+--------------------+\n",
      "|                text|\n",
      "+--------------------+\n",
      "|This is the most ...|\n",
      "|My significant ot...|\n",
      "|We had a tour to ...|\n",
      "|Visited with my w...|\n",
      "|We went in the ni...|\n",
      "|Dont hesitate and...|\n",
      "|I enjoyed the tow...|\n",
      "|Read through the ...|\n",
      "|This by far was o...|\n",
      "|Something you hav...|\n",
      "|The views are bea...|\n",
      "|Worth spending a ...|\n",
      "|Took the tour to ...|\n",
      "|A fantastic fusio...|\n",
      "|Whatever you do i...|\n",
      "|Not to miss..beau...|\n",
      "|We visited in the...|\n",
      "|Go for sunset and...|\n",
      "|We booked weeks a...|\n",
      "|Eiffel Tower is j...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_timestamp, concat_ws, split, hour, explode\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "import sparknlp\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.clustering import LDA\n",
    "\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:4.2.4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read.json('file:///home/ec2-user/eiffel-tower-reviews.json').select('text')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6b0765",
   "metadata": {},
   "source": [
    "# Preprocessamentos\n",
    "\n",
    "Antes de aplicar o LDA, aplicamos um pipeline de processamentos nos dados visando normalizar os dados, remover stopwords (palavra muito comuns de um idioma como artigos, preposições, etc). Essa etapa é importante para garantir que o modelo tenha maior precisão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2ffb34c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                text|              tokens|\n",
      "+--------------------+--------------------+\n",
      "|This is the most ...|[busiest, atttact...|\n",
      "|My significant ot...|[signific, drunke...|\n",
      "|We had a tour to ...|[tour, eiffel, to...|\n",
      "|Visited with my w...|  [visit, wife, son]|\n",
      "|We went in the ni...|[went, night, pm,...|\n",
      "|Dont hesitate and...|[dont, hesit, got...|\n",
      "|I enjoyed the tow...|[enjoi, tower, ki...|\n",
      "|Read through the ...|[read, histori, e...|\n",
      "|This by far was o...|[far, favourit, p...|\n",
      "|Something you hav...|[someth, visit, p...|\n",
      "|The views are bea...|[view, beauti, wo...|\n",
      "|Worth spending a ...|[worth, spend, mi...|\n",
      "|Took the tour to ...|[took, tour, top,...|\n",
      "|A fantastic fusio...|[fantast, fusion,...|\n",
      "|Whatever you do i...|[whatev, afford, ...|\n",
      "|Not to miss..beau...|[missbeauti, plac...|\n",
      "|We visited in the...|[visit, afternoon...|\n",
      "|Go for sunset and...|[go, sunset, stai...|\n",
      "|We booked weeks a...|[book, week, ahea...|\n",
      "|Eiffel Tower is j...|[eiffel, tower, l...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"text\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "# Split sentence to tokens(array)\n",
    "tokenizer = Tokenizer() \\\n",
    "  .setInputCols([\"document\"]) \\\n",
    "  .setOutputCol(\"token\")\n",
    "# clean unwanted characters and garbage\n",
    "normalizer = Normalizer() \\\n",
    "    .setInputCols([\"token\"]) \\\n",
    "    .setOutputCol(\"normalized\")\n",
    "# remove stopwords\n",
    "stopwords_cleaner = StopWordsCleaner()\\\n",
    "      .setInputCols(\"normalized\")\\\n",
    "      .setOutputCol(\"cleanTokens\")\\\n",
    "      .setCaseSensitive(False)\n",
    "# stem the words to bring them to the root form.\n",
    "stemmer = Stemmer() \\\n",
    "    .setInputCols([\"cleanTokens\"]) \\\n",
    "    .setOutputCol(\"stem\")\n",
    "# Finisher is the most important annotator. Spark NLP adds its own structure when we convert each row in the dataframe to document. Finisher helps us to bring back the expected structure viz. array of tokens.\n",
    "finisher = Finisher() \\\n",
    "    .setInputCols([\"stem\"]) \\\n",
    "    .setOutputCols([\"tokens\"]) \\\n",
    "    .setOutputAsArray(True) \\\n",
    "    .setCleanAnnotations(False)\n",
    "# We build a ml pipeline so that each phase can be executed in sequence. This pipeline can also be used to test the model. \n",
    "nlp_pipeline = Pipeline(\n",
    "    stages=[document_assembler, \n",
    "            tokenizer,\n",
    "            normalizer,\n",
    "            stopwords_cleaner, \n",
    "            stemmer, \n",
    "            finisher])\n",
    "# train the pipeline\n",
    "nlp_model = nlp_pipeline.fit(df)\n",
    "# apply the pipeline to transform dataframe.\n",
    "processed_df  = nlp_model.transform(df)\n",
    "# nlp pipeline create intermediary columns that we dont need. So lets select the columns that we need\n",
    "tokens_df = processed_df.select('text', 'tokens')\n",
    "tokens_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c62949f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(inputCol=\"tokens\", outputCol=\"features\", vocabSize=500, minDF=3.0)\n",
    "# train the model\n",
    "cv_model = cv.fit(tokens_df)\n",
    "# transform the data. Output column name will be features.\n",
    "vectorized_tokens = cv_model.transform(tokens_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99618220",
   "metadata": {},
   "source": [
    "Com os dados processados e vetorizados, podemos aplicar o algoritmo LDA para agrupar os dados. Utilizamos aqui n = 4 para obter 4 grupos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "005eac3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/08 20:24:12 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "22/12/08 20:24:12 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n"
     ]
    }
   ],
   "source": [
    "num_topics = 4\n",
    "lda = LDA(k=num_topics, maxIter=15)\n",
    "model = lda.fit(vectorized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "158c4bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic: 0\n",
      "*************************\n",
      "on\n",
      "tower\n",
      "place\n",
      "visit\n",
      "pari\n",
      "eiffel\n",
      "see\n",
      "must\n",
      "landmark\n",
      "great\n",
      "*************************\n",
      "topic: 1\n",
      "*************************\n",
      "tower\n",
      "eiffel\n",
      "pari\n",
      "see\n",
      "go\n",
      "visit\n",
      "time\n",
      "night\n",
      "view\n",
      "must\n",
      "*************************\n",
      "topic: 2\n",
      "*************************\n",
      "ticket\n",
      "get\n",
      "line\n",
      "top\n",
      "view\n",
      "time\n",
      "go\n",
      "wait\n",
      "tower\n",
      "queue\n",
      "*************************\n",
      "topic: 3\n",
      "*************************\n",
      "tower\n",
      "view\n",
      "top\n",
      "go\n",
      "visit\n",
      "eiffel\n",
      "pari\n",
      "night\n",
      "dai\n",
      "amaz\n",
      "*************************\n"
     ]
    }
   ],
   "source": [
    "vocab = cv_model.vocabulary\n",
    "topics = model.describeTopics()   \n",
    "topics_rdd = topics.rdd\n",
    "topics_words = topics_rdd \\\n",
    "       .map(lambda row: row['termIndices']) \\\n",
    "       .map(lambda idx_list: [vocab[idx] for idx in idx_list]) \\\n",
    "       .collect()\n",
    "for idx, topic in enumerate(topics_words):\n",
    "    print(\"topic: {}\".format(idx))\n",
    "    print(\"*\"*25)\n",
    "    for word in topic:\n",
    "       print(word)\n",
    "    print(\"*\"*25)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c667b0b2",
   "metadata": {},
   "source": [
    "Acima, podemos ver os 4 tópicos que foram derivados a partir dos dados. Note que há alguma semântica nos tópicos, por exemplo, o primeiro tópico aparentemente trata sobre a importância de visitar a torre eiffel por ser um grande ponto em Paris, enquanto o segundo tópico fala sobre uma bela vista noturna e o terceiro tópico a respeito de filas, aparentemente para entrar na torre."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
